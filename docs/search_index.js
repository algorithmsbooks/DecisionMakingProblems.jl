var documenterSearchIndex = {"docs":
[{"location":"multicaregiver/#Multi-Caregiver-Crying-Baby","page":"Multi-Caregiver Crying Baby","title":"Multi-Caregiver Crying Baby","text":"","category":"section"},{"location":"multicaregiver/#Problem,-State,-Action-and-Observation-Space","page":"Multi-Caregiver Crying Baby","title":"Problem, State, Action and Observation Space","text":"","category":"section"},{"location":"multicaregiver/","page":"Multi-Caregiver Crying Baby","title":"Multi-Caregiver Crying Baby","text":"The multi-caregiver crying baby problem is a multiagent extension of the crying baby problem. For each caregiver i in mathcalI = 1 2, the states, actions, and observations are as follows:","category":"page"},{"location":"multicaregiver/","page":"Multi-Caregiver Crying Baby","title":"Multi-Caregiver Crying Baby","text":"beginaligned\nmathcalS = texthungry textsated  \nmathcalA^i = textfeed textsing textignore  \nmathcalO^i = textcrying textquiet\nendaligned","category":"page"},{"location":"multicaregiver/#Transition-and-Observation-Dynamics","page":"Multi-Caregiver Crying Baby","title":"Transition and Observation Dynamics","text":"","category":"section"},{"location":"multicaregiver/","page":"Multi-Caregiver Crying Baby","title":"Multi-Caregiver Crying Baby","text":"The transition dynamics are similar to the original crying baby problem, except that either caregiver can feed to satisfy the baby:","category":"page"},{"location":"multicaregiver/","page":"Multi-Caregiver Crying Baby","title":"Multi-Caregiver Crying Baby","text":"beginaligned\nT(textsated mid texthungry(textfeed star)) = T(textsated  texthungry(star textfeed)) = 100\nendaligned","category":"page"},{"location":"multicaregiver/","page":"Multi-Caregiver Crying Baby","title":"Multi-Caregiver Crying Baby","text":"where star indicates all possible other variable assignments. Otherwise, if the actions are not feed, then the baby transitions between sated to hungry as before:","category":"page"},{"location":"multicaregiver/","page":"Multi-Caregiver Crying Baby","title":"Multi-Caregiver Crying Baby","text":"beginaligned\nT(texthungry mid texthungry(star star)) = 100  \nT(textsated mid texthungry(star star)) = 50  \nendaligned","category":"page"},{"location":"multicaregiver/","page":"Multi-Caregiver Crying Baby","title":"Multi-Caregiver Crying Baby","text":"The observation dynamics are also similar to the single agent version, but the model ensures both caregivers make the same observation of the baby, but not necessarily of each other’s choice of caregiving action:","category":"page"},{"location":"multicaregiver/","page":"Multi-Caregiver Crying Baby","title":"Multi-Caregiver Crying Baby","text":"beginaligned\nO((textcry textcry) mid (textsing star) texthungry) = O((textcry textcry) mid (star textsing) texthungry) = 90 \nO((textquiet textquiet) mid (textsing star) texthungry) = O((textquiet textquiet) mid (star textsing) texthungry) = 10 \nO((textcry textcry) mid (textsing star) textstaed) = O((textcry textcry) mid (star textsing) textsated) = 0 \nendaligned","category":"page"},{"location":"multicaregiver/","page":"Multi-Caregiver Crying Baby","title":"Multi-Caregiver Crying Baby","text":"If the actions are not sing, then the observations are as follows:","category":"page"},{"location":"multicaregiver/","page":"Multi-Caregiver Crying Baby","title":"Multi-Caregiver Crying Baby","text":"beginaligned\nO((textcry textcry) mid (star star) texthungry) = 90  \nO((textquiet textquiet) mid (star star) texthungry) = 10  \nO((textcry textcry) mid (star star) textsated) = 0  \nO((textquiet textquiet) mid (star star) textsated) = 100  \nendaligned","category":"page"},{"location":"multicaregiver/#Reward","page":"Multi-Caregiver Crying Baby","title":"Reward","text":"","category":"section"},{"location":"multicaregiver/","page":"Multi-Caregiver Crying Baby","title":"Multi-Caregiver Crying Baby","text":"Both caregivers want to help the baby when the baby is hungry, assigning the same penalty of 100 for both. However, the first caregiver favors feeding and the second caregiver favors singing. For feeding, the first caregiver receives an extra penalty of only 25, while the second caregiver receives an extra penalty of 50. For signing, the first caregiver is penalized by 05, while the second caregiver is penalized by only 025.","category":"page"},{"location":"catch/#Catch","page":"Catch","title":"Catch","text":"","category":"section"},{"location":"catch/#Problem","page":"Catch","title":"Problem","text":"","category":"section"},{"location":"catch/","page":"Catch","title":"Catch","text":"In the catch problem, Johnny would like to successfully catch throws from his father, and he prefers catching longer-distance throws. However, he is uncertain about the relationship between the distances of a throw and the probability of a successful catch. He does know that the probability of a successful catch is the same regardless of whether he is throwing or catching, and he has a finite number of attempted catches to maximize his expected utility before he has to go home.","category":"page"},{"location":"catch/","page":"Catch","title":"Catch","text":"As shown in the figure below, Johnny models the probability of successfully catching a ball thrown a distance d as","category":"page"},{"location":"catch/","page":"Catch","title":"Catch","text":"P(textcatch mid d) = 1 - frac11 + textexp(-fracd-s15)","category":"page"},{"location":"catch/","page":"Catch","title":"Catch","text":"where the proficiency s is unknown and does not change over time. To keep things manageable, he assumes s belongs to the discrete set mathcalS = 20 40 60 80.","category":"page"},{"location":"catch/","page":"Catch","title":"Catch","text":"(Image: Visualization of Catch Distributions)","category":"page"},{"location":"catch/#Reward-and-Action-Space","page":"Catch","title":"Reward and Action Space","text":"","category":"section"},{"location":"catch/","page":"Catch","title":"Catch","text":"The reward for a successful catch is equal to the distance. If the catch is unsuccessful, then the reward is zero. Johnny wants to maximize the reward over a finite number of attempted throws. With each throw, Johnny chooses a distance from a discrete set mathcalA = 10 20 dots 100. Johnny begins with a uniform distribution over mathcalS.","category":"page"},{"location":"machine_replacement/#Machine-Replacement","page":"Machine Replacement","title":"Machine Replacement","text":"","category":"section"},{"location":"machine_replacement/#Problem","page":"Machine Replacement","title":"Problem","text":"","category":"section"},{"location":"machine_replacement/","page":"Machine Replacement","title":"Machine Replacement","text":"The machine replacement problem is a discrete POMDP in which we maintain a machine that produces products. This problem is used for its relative simplicity and the varied size and shape of the optimal policy regions.","category":"page"},{"location":"machine_replacement/","page":"Machine Replacement","title":"Machine Replacement","text":"The machine produces products for us when it is working properly. Over time, the two primary components in the machine may break down, together or individually, leading to defective product. We can indirectly observe whether the machine is faulty by examining the products, or by directly examining the components in the machine.","category":"page"},{"location":"machine_replacement/#State,-Action-and-Observation-Space","page":"Machine Replacement","title":"State, Action and Observation Space","text":"","category":"section"},{"location":"machine_replacement/","page":"Machine Replacement","title":"Machine Replacement","text":"The problem has states mathcalS = 0 1 2, corresponding to the number of faulty internal components. There are four actions, used prior to each production cycle:","category":"page"},{"location":"machine_replacement/","page":"Machine Replacement","title":"Machine Replacement","text":"manufacture, manufacture product and do not examine the product,\nexamine, manufacture product and examine the product,\ninterrupt, interrupt production, inspect, and replace failed components, and\nreplace replace both components after interrupting production.","category":"page"},{"location":"machine_replacement/","page":"Machine Replacement","title":"Machine Replacement","text":"When we examine the product, we can observe whether or not it is defective. All other actions only observe non-defective products.","category":"page"},{"location":"machine_replacement/#Transitions,-Reward-and-Observation-Functions","page":"Machine Replacement","title":"Transitions, Reward and Observation Functions","text":"","category":"section"},{"location":"machine_replacement/","page":"Machine Replacement","title":"Machine Replacement","text":"The components in the machine independently have a 10  chance of breaking down with each production cycle. Each failed component contributes a 50  chance of producing a defective product. A nondefective product nets 1 reward, whereas a defective product nets 0 reward. The transition dynamics assume that component breakdown is determined before a product is made, so the manufacture action on a fully-functional machine does not have a 100  chance of producing 1 reward.","category":"page"},{"location":"machine_replacement/","page":"Machine Replacement","title":"Machine Replacement","text":"The manufacture action incurs no penalty. Examining the product costs 025. Interrupting the line costs 05 to inspect the machine, causes no product to be produced, and incurs 1 for each broken component. Simply replacing both components always incurs 2, but does not have an inspection cost.","category":"page"},{"location":"machine_replacement/","page":"Machine Replacement","title":"Machine Replacement","text":"The transition, observation, and reward functions are given in the table below.","category":"page"},{"location":"machine_replacement/#Optimal-Policies","page":"Machine Replacement","title":"Optimal Policies","text":"","category":"section"},{"location":"machine_replacement/","page":"Machine Replacement","title":"Machine Replacement","text":"Optimal policies for increasing horizons are shown in the figures below:","category":"page"},{"location":"machine_replacement/","page":"Machine Replacement","title":"Machine Replacement","text":"(Image: Visualization of Optimal Policies)","category":"page"},{"location":"machine_replacement/","page":"Machine Replacement","title":"Machine Replacement","text":"(Image: Visualization of Optimal Policies2)","category":"page"},{"location":"collision_avoidance/#Aircraft-Collision-Avoidance","page":"Aircraft Collision Avoidance","title":"Aircraft Collision Avoidance","text":"","category":"section"},{"location":"collision_avoidance/#Problem","page":"Aircraft Collision Avoidance","title":"Problem","text":"","category":"section"},{"location":"collision_avoidance/","page":"Aircraft Collision Avoidance","title":"Aircraft Collision Avoidance","text":"The aircraft collision avoidance problem involves deciding when to issue a climb or descend advisory to our aircraft to avoid an intruder aircraft. The figure below illustrates the problem scene.","category":"page"},{"location":"collision_avoidance/","page":"Aircraft Collision Avoidance","title":"Aircraft Collision Avoidance","text":"(Image: Visualization of Aircraft Collision Avoidance)","category":"page"},{"location":"collision_avoidance/#State-and-Action-Space","page":"Aircraft Collision Avoidance","title":"State and Action Space","text":"","category":"section"},{"location":"collision_avoidance/","page":"Aircraft Collision Avoidance","title":"Aircraft Collision Avoidance","text":"There are three actions corresponding to no advisory, commanding a 5 text ms descend, and commanding a 5 text ms climb. The intruder is approaching us head-on with constant horizontal closing speed. The state is specified by the altitude h of our aircraft measured relative to the intruder aircraft, our vertical rate ˙h, the previous action a_textprev, and the time to potential collision t_textcol.","category":"page"},{"location":"collision_avoidance/#Transitions","page":"Aircraft Collision Avoidance","title":"Transitions","text":"","category":"section"},{"location":"collision_avoidance/","page":"Aircraft Collision Avoidance","title":"Aircraft Collision Avoidance","text":"Given action a, the state variables are updated as follows:","category":"page"},{"location":"collision_avoidance/","page":"Aircraft Collision Avoidance","title":"Aircraft Collision Avoidance","text":"beginaligned\nh rightarrow h + h Delta t \nh rightarrow (h + v)Delta t \na_textprev rightarrow a \nt_textcol rightarrow t_textcol - Delta t\nendaligned","category":"page"},{"location":"collision_avoidance/","page":"Aircraft Collision Avoidance","title":"Aircraft Collision Avoidance","text":"where Delta t = 1 text s and v is selected from a discrete distribution over -2 0 or 2 textms^2 with associated probabilities 025 05 025. The value h is given by","category":"page"},{"location":"collision_avoidance/","page":"Aircraft Collision Avoidance","title":"Aircraft Collision Avoidance","text":"h = begincases 0  textif  a = text no advisory  aDelta t  textif  a - hDelta t  h_textlimit  textsign(a - h)h_textlimit  textotherwise endcases","category":"page"},{"location":"collision_avoidance/","page":"Aircraft Collision Avoidance","title":"Aircraft Collision Avoidance","text":"where h_textlimit = 1 textms^2.","category":"page"},{"location":"collision_avoidance/#Reward-and-Termination-Condition","page":"Aircraft Collision Avoidance","title":"Reward and Termination Condition","text":"","category":"section"},{"location":"collision_avoidance/","page":"Aircraft Collision Avoidance","title":"Aircraft Collision Avoidance","text":"The episode terminates when taking an action when t_textcol  0. There is a penalty of 1 when the intruder comes within 50 m when t_textcol = 0, and there is a penalty of 001 when a neq a_textprev.","category":"page"},{"location":"collision_avoidance/#Strategies","page":"Aircraft Collision Avoidance","title":"Strategies","text":"","category":"section"},{"location":"collision_avoidance/","page":"Aircraft Collision Avoidance","title":"Aircraft Collision Avoidance","text":"The aircraft collision avoidance problem can be efficiently solved over a discretized grid using backwards induction value iteration (Section 7.6 of Algorithms for Decision Making) because the dynamics deterministically reduce t_textcol. Slices of the optimal value function and policy are depicted below.","category":"page"},{"location":"collision_avoidance/","page":"Aircraft Collision Avoidance","title":"Aircraft Collision Avoidance","text":"(Image: Slices of the optimal value function)","category":"page"},{"location":"pomg/#POMG-Usage","page":"POMG Usage","title":"POMG Usage","text":"","category":"section"},{"location":"pomg/#Partially-Observable-Markov-Game","page":"POMG Usage","title":"Partially Observable Markov Game","text":"","category":"section"},{"location":"pomg/","page":"POMG Usage","title":"POMG Usage","text":"The POMG struct gives the following objects:","category":"page"},{"location":"pomg/","page":"POMG Usage","title":"POMG Usage","text":"γ: discount factor\nℐ: agents\n𝒮: state space\n𝒜: joint action space\n𝒪: joint observation space\nT: transition function\nO: joint observation function\nR: joint reward function","category":"page"},{"location":"pomg/","page":"POMG Usage","title":"POMG Usage","text":"The agents ℐ are the players of the game. The joint action space 𝒜 is the set of all possible ordered pairs of actions amongst all of the agents. The joint observation space 𝒪 is the set of all possible joint observations. The transition function takes in a state s in 𝒮, a joint action a and a new state s'and returns the transition probability of going from s to s' by taking action a. The joint observation function takes in a state, s, a joint action, a, and a joint observation o in 𝒪 and returns a probability of observing o by taking action a from state s. The joint reward function R takes a state and a joint action in 𝒜 and returns a reward value.","category":"page"},{"location":"2048/#","page":"2048","title":"2048","text":"","category":"section"},{"location":"2048/#Problem","page":"2048","title":"Problem","text":"","category":"section"},{"location":"2048/","page":"2048","title":"2048","text":"The 2048 problem is based on a popular tile game.","category":"page"},{"location":"2048/#States,-Actions-and-Transitions","page":"2048","title":"States, Actions and Transitions","text":"","category":"section"},{"location":"2048/","page":"2048","title":"2048","text":"It has discrete state and action spaces. The game is played on a 4 × 4 board. The board is initially empty except for two tiles, each of which can have value 2 or 4. A randomly selected starting state is shown in the figure below.","category":"page"},{"location":"2048/","page":"2048","title":"2048","text":"(Image: Starting State)","category":"page"},{"location":"2048/","page":"2048","title":"2048","text":"The agent can move all tiles left, down, right, or up. Choosing a direction pushes all tiles in that direction. A tile stops when it hits a wall or another tile of a different value. A tile that hits another tile of the same value merges with that tile, forming a new tile with their combined value. After shifting and merging, a new tile of value 2 or 4 is spawned in a random open space. This process is shown in the figure below.","category":"page"},{"location":"2048/","page":"2048","title":"2048","text":"(Image: Transition)","category":"page"},{"location":"2048/#Reward-Functions-and-Termination","page":"2048","title":"Reward Functions and Termination","text":"","category":"section"},{"location":"2048/","page":"2048","title":"2048","text":"The game ends when we can no longer shift tiles to produce an empty space. Rewards are only obtained when merging two tiles and are equal to the merge tile’s value. An example state-action transition with a merge is shown in figure below.","category":"page"},{"location":"2048/","page":"2048","title":"2048","text":"(Image: Visualization of Optimal Policies)","category":"page"},{"location":"2048/#Optimal-Policies","page":"2048","title":"Optimal Policies","text":"","category":"section"},{"location":"2048/","page":"2048","title":"2048","text":"A common strategy is to choose a corner and alternate between the two actions that lead in that direction. This tends to stratify the tiles such that the larger-valued ones are in the corner and the newly spawned tiles are in the periphery.","category":"page"},{"location":"pomdp/#POMDP-Usage","page":"POMDP Usage","title":"POMDP Usage","text":"","category":"section"},{"location":"pomdp/#POMDP","page":"POMDP Usage","title":"POMDP","text":"","category":"section"},{"location":"pomdp/","page":"POMDP Usage","title":"POMDP Usage","text":"The MDP struct gives the following:","category":"page"},{"location":"pomdp/","page":"POMDP Usage","title":"POMDP Usage","text":"γ: discount factor\n𝒮: state space\n𝒜: action space\n𝒪: observation space\nT: transition function\nR: reward function\nO: observation function\nTRO: function that allows us to sample transition, reward, and observation","category":"page"},{"location":"pomdp/","page":"POMDP Usage","title":"POMDP Usage","text":"The function T takes in a state s and an action a and returns a distribution of possible states. The reward function R takes in a state s and action a and returns an reward. The observation function O takes in a state s and an action a and returns a distribution of possible observations. Finally TRO takes in a state s and an action a and returns a tuple (s', r, o) where s' is the new state sampled from the transition function, r is the reward and o is an observation sampled from the observation function.","category":"page"},{"location":"decpomdp/#DecPOMDP-Usage","page":"DecPOMDP Usage","title":"DecPOMDP Usage","text":"","category":"section"},{"location":"decpomdp/#Decentralized-POMDP","page":"DecPOMDP Usage","title":"Decentralized POMDP","text":"","category":"section"},{"location":"decpomdp/","page":"DecPOMDP Usage","title":"DecPOMDP Usage","text":"The DecPOMDP struct gives the following objects:","category":"page"},{"location":"decpomdp/","page":"DecPOMDP Usage","title":"DecPOMDP Usage","text":"γ: discount factor\nℐ: agents\n𝒮: state space\n𝒜: joint action space\n𝒪: joint observation space\nT: transition function\nO: joint observation function\nR: joint reward function","category":"page"},{"location":"decpomdp/","page":"DecPOMDP Usage","title":"DecPOMDP Usage","text":"The agents ℐ are the players of the game. The joint action space 𝒜 is the set of all possible ordered pairs of actions amongst all of the agents. The joint observation space 𝒪 is the set of all possible joint observations. The transition function takes in a state s in 𝒮, a joint action a and a new state s'and returns the transition probability of going from s to s' by taking action a. The joint observation function takes in a state, s, a joint action, a, and a joint observation o in 𝒪 and returns a probability of observing o by taking action a from state s. The joint reward function R takes a state and a joint action in 𝒜 and returns a reward value.","category":"page"},{"location":"predator_prey/#Predatory-Prey-Hex-World","page":"Predatory-Prey Hex World","title":"Predatory-Prey Hex World","text":"","category":"section"},{"location":"predator_prey/#Problem","page":"Predatory-Prey Hex World","title":"Problem","text":"","category":"section"},{"location":"predator_prey/","page":"Predatory-Prey Hex World","title":"Predatory-Prey Hex World","text":"The predator-prey hex world problem expands the hex world dynamics to include multiple agents consisting of predators and prey. A predator tries to capture a prey as quickly as possible, and a prey tries to escape the predators as long as possible. The initial state of the hex world is shown in the figure below. There are no terminal states in this game.","category":"page"},{"location":"predator_prey/","page":"Predatory-Prey Hex World","title":"Predatory-Prey Hex World","text":"(Image: Visualization of Predator Prey)","category":"page"},{"location":"predator_prey/#State-and-Action-Space","page":"Predatory-Prey Hex World","title":"State and Action Space","text":"","category":"section"},{"location":"predator_prey/","page":"Predatory-Prey Hex World","title":"Predatory-Prey Hex World","text":"There is a set of predators mathcalI_textpred and a set of prey mathcalI_textprey, with mathcalI = mathcalI_textpred cup mathcalI_textprey. The states contain the locations of each agent: mathcalS = mathcalS^1 times dots times mathcalS^mathcalI, with each mathcalS^i equal to all hex locations. The joint action space is mathcalA = mathcalA^1 times dots times mathcalA^mathcalI, where each mathcalA^i consists of all six hex directions of movement.","category":"page"},{"location":"predator_prey/#Transitions-and-Rewards","page":"Predatory-Prey Hex World","title":"Transitions and Rewards","text":"","category":"section"},{"location":"predator_prey/","page":"Predatory-Prey Hex World","title":"Predatory-Prey Hex World","text":"If a predator i in mathcalI_textpred and prey j in mathcalI_textprey share the same hex with s_i = s_j, then the prey is devoured. The prey j is then transported to a random hex cell, representing its offspring appearing in the world. Otherwise, the state transitions are independent and are as described in the original hex world.","category":"page"},{"location":"predator_prey/","page":"Predatory-Prey Hex World","title":"Predatory-Prey Hex World","text":"One or more predators can capture one or more prey if they all happen to be in the same cell. If n predators and m prey all share the same cell, the predators receive a reward of mn. For example, if two predators capture one prey together, they each get a reward of 12. If three predators capture five prey together, they each get a reward of 53. Moving predators receive unit penalty. Prey can move with no penalty, but they receive a penalty of 100 for being devoured.","category":"page"},{"location":"travelers/#Traveler's-Problem","page":"Traveler's Problem","title":"Traveler's Problem","text":"","category":"section"},{"location":"travelers/#Problem-and-Action-Space","page":"Traveler's Problem","title":"Problem and Action Space","text":"","category":"section"},{"location":"travelers/","page":"Traveler's Problem","title":"Traveler's Problem","text":"The traveler’s dilemma is a game where an airline loses two identical suitcases from two travelers. The airline asks the travelers to write down the value of their suitcases, which can be between  2 and  100, inclusive.","category":"page"},{"location":"travelers/#Reward-Function","page":"Traveler's Problem","title":"Reward Function","text":"","category":"section"},{"location":"travelers/","page":"Traveler's Problem","title":"Traveler's Problem","text":"If both put down the same value, then they both get that value. The traveler with the lower value gets their value plus  2. The traveler with the higher value gets the lower value minus  2. In other words, the reward function is as follows:","category":"page"},{"location":"travelers/","page":"Traveler's Problem","title":"Traveler's Problem","text":"beginaligned\nR_i(a_i a_-i) = begincases a_i  textif  a_i = a_-i  a_i + 2  textif  a_i  a_-i  a_-i - 2  textotherwise endcases\nendaligned","category":"page"},{"location":"travelers/#Optimal-Policy","page":"Traveler's Problem","title":"Optimal Policy","text":"","category":"section"},{"location":"travelers/","page":"Traveler's Problem","title":"Traveler's Problem","text":"Most people tend to put down between  97 and  100. However, somewhat counter-intuitively, there is a unique Nash equilibrium of only  2.","category":"page"},{"location":"mdp/#MDP-Usage","page":"MDP Usage","title":"MDP Usage","text":"","category":"section"},{"location":"mdp/#MDP","page":"MDP Usage","title":"MDP","text":"","category":"section"},{"location":"mdp/","page":"MDP Usage","title":"MDP Usage","text":"The MDP struct gives the following:","category":"page"},{"location":"mdp/","page":"MDP Usage","title":"MDP Usage","text":"γ: discount factor\n𝒮: state space\n𝒜: action space\nT: transition function\nR: reward function\nTR: function allows us to sample transition and reward","category":"page"},{"location":"mdp/","page":"MDP Usage","title":"MDP Usage","text":"The function T takes in a state s and an action a and returns a distribution of states which can be sampled. The reward function R takes in a state s and action a and returns an reward. Finally TR takes in a state s and an action a and returns a tuple (s', r) where s' is the new state sampled from the transition function and r is the reward.","category":"page"},{"location":"mountain_car/#Mountain-Car","page":"Mountain Car","title":"Mountain Car","text":"","category":"section"},{"location":"mountain_car/#Problem","page":"Mountain Car","title":"Problem","text":"","category":"section"},{"location":"mountain_car/","page":"Mountain Car","title":"Mountain Car","text":"In the mountain car problem, a vehicle must drive to the right, out of a valley. The valley walls are steep enough that blindly accelerating toward the goal with insufficient speed causes the vehicle to come to a halt and slide back down. The agent must learn to accelerate left first, in order to gain enough momentum on the return to make it up the hill.","category":"page"},{"location":"mountain_car/","page":"Mountain Car","title":"Mountain Car","text":"The mountain car problem is a good example of a problem with delayed return. Many actions are required to get to the goal state, making it difficult for an untrained agent to receive anything other than consistent unit penalties. The best learning algorithms are able to efficiently propagate knowledge from trajectories that reach the goal back to the rest of the state space.","category":"page"},{"location":"mountain_car/","page":"Mountain Car","title":"Mountain Car","text":"(Image: Visualization of Mountain Car)","category":"page"},{"location":"mountain_car/#State-and-Action-Space","page":"Mountain Car","title":"State and Action Space","text":"","category":"section"},{"location":"mountain_car/","page":"Mountain Car","title":"Mountain Car","text":"The state space, action space and observation space are","category":"page"},{"location":"mountain_car/","page":"Mountain Car","title":"Mountain Car","text":"beginaligned\nmathcalS = -12 06 times -007 007 \nmathcalA = -1 0 1 \nendaligned","category":"page"},{"location":"mountain_car/","page":"Mountain Car","title":"Mountain Car","text":"The state is the vehicle’s horizontal position x in 12 06 and speed v in 007 007 At any given time step, the vehicle can accelerate left (a = 1), accelerate right (a = 1), or coast (a = 0).","category":"page"},{"location":"mountain_car/#Transitions","page":"Mountain Car","title":"Transitions","text":"","category":"section"},{"location":"mountain_car/","page":"Mountain Car","title":"Mountain Car","text":"Transitions in the mountain car problem are deterministic:","category":"page"},{"location":"mountain_car/","page":"Mountain Car","title":"Mountain Car","text":"beginaligned\nv rightarrow v + 0001a - 00025 cos(3x) \nx rightarrow x + v\nendaligned","category":"page"},{"location":"mountain_car/","page":"Mountain Car","title":"Mountain Car","text":"The gravitational term in the speed update is what drives the under-powered vehicle back toward the valley floor. Transitions are clamped to the bounds of the state-space.","category":"page"},{"location":"mountain_car/","page":"Mountain Car","title":"Mountain Car","text":"A visualization of the problem is shown below:","category":"page"},{"location":"mountain_car/#Reward-Function-and-Termination-Condition","page":"Mountain Car","title":"Reward Function and Termination Condition","text":"","category":"section"},{"location":"mountain_car/","page":"Mountain Car","title":"Mountain Car","text":"We receive 1 reward every turn, and terminate when the vehicle makes it up the right side of the valley past x = 06.","category":"page"},{"location":"cart_pole/#Cart-Pole","page":"Cart-Pole","title":"Cart-Pole","text":"","category":"section"},{"location":"cart_pole/#Problem","page":"Cart-Pole","title":"Problem","text":"","category":"section"},{"location":"cart_pole/","page":"Cart-Pole","title":"Cart-Pole","text":"The cart-pole problem, also sometimes called the pole balancing problem, has the agent move a cart back and forth. As shown in the figure below, this cart has a rigid pole attached to it by a swivel, such that as the cart moves back and forth, the pole begins to rotate. The objective is to the keep the pole vertically balanced while keeping the cart near within the allowed lateral bounds.","category":"page"},{"location":"cart_pole/","page":"Cart-Pole","title":"Cart-Pole","text":"(Image: Visualization of Cart-Pole)","category":"page"},{"location":"cart_pole/#State-and-Action-Space","page":"Cart-Pole","title":"State and Action Space","text":"","category":"section"},{"location":"cart_pole/","page":"Cart-Pole","title":"Cart-Pole","text":"The actions are to either apply a left or right force F on the cart. The state space is defined by four continuous variables: the lateral position of the cart x, its lateral velocity v, the angle of the pole theta, and the pole’s angular velocity omega. The problem involves a variety of parameters including the mass of the cart m_textcart, the mass of the pole m_textpole, the pole length ell, the force magnitude F, gravitational acceleration g, the timestep Delta t, the maximum x deviation, the maximum angular deviation, and friction losses between the cart and pole or between the cart and its track.","category":"page"},{"location":"cart_pole/","page":"Cart-Pole","title":"Cart-Pole","text":"The cart-pole problem is typically initialized with each random value drawn from mathcalU(-005 005).","category":"page"},{"location":"cart_pole/#Transitions","page":"Cart-Pole","title":"Transitions","text":"","category":"section"},{"location":"cart_pole/","page":"Cart-Pole","title":"Cart-Pole","text":"Given an input force F, the angular acceleration on the pole is","category":"page"},{"location":"cart_pole/","page":"Cart-Pole","title":"Cart-Pole","text":"alpha = fracg sin(theta) - tau cos(theta)fracell2 left(frac43 - fracm_textpolem_textcart + m_textpole cos(theta)^2right)","category":"page"},{"location":"cart_pole/","page":"Cart-Pole","title":"Cart-Pole","text":"where","category":"page"},{"location":"cart_pole/","page":"Cart-Pole","title":"Cart-Pole","text":"beginaligned\ntau = fracF + omega^2 ell sin(theta2)m_textcart + m_textpole\nendaligned","category":"page"},{"location":"cart_pole/","page":"Cart-Pole","title":"Cart-Pole","text":"The lateral cart acceleration is","category":"page"},{"location":"cart_pole/","page":"Cart-Pole","title":"Cart-Pole","text":"beginaligned\na = tau - fracell2 alpha cos(theta) fracm_textpolem_textcart + m_textpole\nendaligned","category":"page"},{"location":"cart_pole/","page":"Cart-Pole","title":"Cart-Pole","text":"The state is updated with Euler integration:","category":"page"},{"location":"cart_pole/","page":"Cart-Pole","title":"Cart-Pole","text":"beginaligned\nx rightarrow x + v Delta t \nv rightarrow v + a Delta t \ntheta rightarrow theta + omega Delta t \nomega rightarrow omega + alpha Delta t  \nendaligned","category":"page"},{"location":"cart_pole/#Reward-and-Termination-Condition","page":"Cart-Pole","title":"Reward and Termination Condition","text":"","category":"section"},{"location":"cart_pole/","page":"Cart-Pole","title":"Cart-Pole","text":"Since the objective is to the keep the pole vertically balanced while keeping the cart near within the allowed lateral bounds, 1 reward is obtained each time step in which these conditions are met, and transition to a terminal zero-reward state occurs whenever they are not.","category":"page"},{"location":"collab_predator_prey/#Collaborative-Predatory-Prey-Hex-World","page":"Collaborative Predatory-Prey Hex World","title":"Collaborative Predatory-Prey Hex World","text":"","category":"section"},{"location":"collab_predator_prey/#Problem","page":"Collaborative Predatory-Prey Hex World","title":"Problem","text":"","category":"section"},{"location":"collab_predator_prey/","page":"Collaborative Predatory-Prey Hex World","title":"Collaborative Predatory-Prey Hex World","text":"The collaborative predator-prey hex world is an variant of the predator-prey hex world in which a team of predators chase a single moving prey. The predators must work together to capture a prey. The prey moves randomly to a neighboring cell that is not occupied by a predator.","category":"page"},{"location":"collab_predator_prey/#Observations-and-Rewards","page":"Collaborative Predatory-Prey Hex World","title":"Observations and Rewards","text":"","category":"section"},{"location":"collab_predator_prey/","page":"Collaborative Predatory-Prey Hex World","title":"Collaborative Predatory-Prey Hex World","text":"Predators also only make noisy local observations of the environment. Each predator i detects whether a prey is within a neighboring cell mathcalO^i = textprey textnothing. The predators are penalized with a 1 reward for movement. They receive a reward of 10 if one or more of them capture the prey, meaning they are in the same cell as the prey. At this point, the prey is randomly assigned a new cell, signifying the arrival of a new prey for the predators to begin hunting.","category":"page"},{"location":"hexworld/#Hex-World","page":"Hex World","title":"Hex World","text":"","category":"section"},{"location":"hexworld/#Problem","page":"Hex World","title":"Problem","text":"","category":"section"},{"location":"hexworld/","page":"Hex World","title":"Hex World","text":"The hex world problem is a simple MDP in which we must traverse a tile map to reach a goal state.","category":"page"},{"location":"hexworld/#State-Space,-Action-Space-and-Transitions","page":"Hex World","title":"State Space, Action Space and Transitions","text":"","category":"section"},{"location":"hexworld/","page":"Hex World","title":"Hex World","text":"Each cell in the tile map represents a state in the MDP. We can attempt to move in any of the 6 directions. The effects of these actions are stochastic. As shown in the figure below, we move 1 step in the specified direction with probability 07, and we move 1 step in one of the neighboring directions, each with probability 015. If we bump against the outer border of the grid, then we do not move at all, at cost 10.","category":"page"},{"location":"hexworld/","page":"Hex World","title":"Hex World","text":"(Image: Visualization of HexWorld Actions)","category":"page"},{"location":"hexworld/#Reward-and-Termination-Condition","page":"Hex World","title":"Reward and Termination Condition","text":"","category":"section"},{"location":"hexworld/","page":"Hex World","title":"Hex World","text":"Certain cells in the hex world problem are terminal states. Taking any action in these cells gives us a specified reward and then transports us to a terminal state. No further reward is received in the terminal state. The total number of states in the hex world problem is thus the number of tiles plus 1, for the terminal state.","category":"page"},{"location":"hexworld/#Optimal-Policies","page":"Hex World","title":"Optimal Policies","text":"","category":"section"},{"location":"hexworld/","page":"Hex World","title":"Hex World","text":"The figure below shows an optimal policy for two hex world problem configurations.","category":"page"},{"location":"hexworld/","page":"Hex World","title":"Hex World","text":"(Image: Optimal HexWorld Policy1)","category":"page"},{"location":"hexworld/","page":"Hex World","title":"Hex World","text":"(Image: Optimal HexWorld Policy2)","category":"page"},{"location":"hexworld/","page":"Hex World","title":"Hex World","text":"(Image: Optimal HexWorld Policy3)","category":"page"},{"location":"simplegame/#SimpleGame-Usage","page":"SimpleGame Usage","title":"SimpleGame Usage","text":"","category":"section"},{"location":"simplegame/#Simple-Game","page":"SimpleGame Usage","title":"Simple Game","text":"","category":"section"},{"location":"simplegame/","page":"SimpleGame Usage","title":"SimpleGame Usage","text":"The SimpleGame struct gives the following objects:","category":"page"},{"location":"simplegame/","page":"SimpleGame Usage","title":"SimpleGame Usage","text":"γ: discount factor\nℐ: agents\n𝒜: joint action space\nR: joint reward function","category":"page"},{"location":"simplegame/","page":"SimpleGame Usage","title":"SimpleGame Usage","text":"The agents ℐ in a simple game are the players of the game. The joint action space 𝒜 is the set of all possible ordered pairs of actions amongst all of the agents. The joint reward function R takes a joint action in 𝒜 and returns a reward value.","category":"page"},{"location":"simple_lqr/#Simple-Regulator","page":"Simple Regulator","title":"Simple Regulator","text":"","category":"section"},{"location":"simple_lqr/#Problem,-State-and-Action-Space","page":"Simple Regulator","title":"Problem, State and Action Space","text":"","category":"section"},{"location":"simple_lqr/","page":"Simple Regulator","title":"Simple Regulator","text":"The simple regulator problem is a simple linear quadratic regulator problem with a single state. It is an MDP with a single real-valued state and a single real-valued action.","category":"page"},{"location":"simple_lqr/#Transitions-and-Rewards","page":"Simple Regulator","title":"Transitions and Rewards","text":"","category":"section"},{"location":"simple_lqr/","page":"Simple Regulator","title":"Simple Regulator","text":"Transitions are linear Gaussian such that a successor state s is drawn from the Gaussian distribution mathcalN(s + a 012). Rewards are quadratic, R(s a) = s^2, and do not depend on the action. The examples in this text use the initial state distribution mathcalN(03 012). \u0001","category":"page"},{"location":"simple_lqr/#Optimal-Policies","page":"Simple Regulator","title":"Optimal Policies","text":"","category":"section"},{"location":"simple_lqr/","page":"Simple Regulator","title":"Simple Regulator","text":"Optimal finite-horizon policies cannot be derivied using the methods from section 7.8 of Algorithms for Decision Making. In this case, T_s = 1 T_a = 1 R_s = 1 R_a = 0 and w is drawn from mathcalN(0 012). Applications of the Riccati equation require that R_a be negative definite, which it is not.","category":"page"},{"location":"simple_lqr/","page":"Simple Regulator","title":"Simple Regulator","text":"The optimal policy is pi(s) = s, resulting in a successor state distribution centered at the origin. In the policy gradient chapters we often learn parameterized policies of the form pi_theta(s) = mathcalN(theta_1 s theta_2^2). In such cases, the optimal parameterization for the simple regulator problem is theta_1 = 1 and theta_2 is asymptotically close to zero.","category":"page"},{"location":"simple_lqr/","page":"Simple Regulator","title":"Simple Regulator","text":"The optimal value function for the simple regulator problem is also centered about the origin, with reward decreasing quadratically:","category":"page"},{"location":"simple_lqr/","page":"Simple Regulator","title":"Simple Regulator","text":"beginaligned\nmathcalU(s) = -s^2 + fracgamma1 + gamma mathbbE_s sim mathcalN(0 01^2) left-s^2 right \napprox -s^2 - 0010fracgamma1 + gamma\nendaligned","category":"page"},{"location":"crying_baby/#Crying-Baby","page":"Crying Baby","title":"Crying Baby","text":"","category":"section"},{"location":"crying_baby/#Problem","page":"Crying Baby","title":"Problem","text":"","category":"section"},{"location":"crying_baby/","page":"Crying Baby","title":"Crying Baby","text":"The crying baby problem is a simple POMDP with two states, three actions, and two observations. Our goal is to care for a baby, and we do so by choosing at each time step whether to feed the baby, sing to the baby, or ignore the baby.","category":"page"},{"location":"crying_baby/","page":"Crying Baby","title":"Crying Baby","text":"The baby becomes hungry over time. We do not directly observe whether the baby is hungry, but instead receive a noisy observation in the form of whether or not the baby is crying.","category":"page"},{"location":"crying_baby/#State,-Action-and-Observation-Space","page":"Crying Baby","title":"State, Action and Observation Space","text":"","category":"section"},{"location":"crying_baby/","page":"Crying Baby","title":"Crying Baby","text":"The state space, action space and observation space are","category":"page"},{"location":"crying_baby/","page":"Crying Baby","title":"Crying Baby","text":"beginaligned\nmathcalS = texthungry textsated \nmathcalA = textfeed textsing textignore \nmathcalO = textcrying textquiet\nendaligned","category":"page"},{"location":"crying_baby/#Transitions","page":"Crying Baby","title":"Transitions","text":"","category":"section"},{"location":"crying_baby/","page":"Crying Baby","title":"Crying Baby","text":"Feeding will always sate the baby. Ignoring the baby risks a sated baby becoming hungry, and ensures that a hungry baby remains hungry. Singing to the baby is an information gathering action with the same transition dynamics as ignoring, but without the potential for crying when sated (not hungry) and with an increased chance of crying when hungry.","category":"page"},{"location":"crying_baby/","page":"Crying Baby","title":"Crying Baby","text":"The transition dynamics are:","category":"page"},{"location":"crying_baby/","page":"Crying Baby","title":"Crying Baby","text":"beginaligned\nT(textsated mid texthungry textfeed) = 100 \nT(texthungry mid texthungry textsing) = 100 \nT(texthungry mid texthungry textignore) = 100 \nT(textsated mid textsated textfeed) = 100 \nT(texthungry mid textsated textsing) = 10 \nT(texthungry mid textsated textignore) = 10\nendaligned","category":"page"},{"location":"crying_baby/#Observations","page":"Crying Baby","title":"Observations","text":"","category":"section"},{"location":"crying_baby/","page":"Crying Baby","title":"Crying Baby","text":"The observation dynamics are:","category":"page"},{"location":"crying_baby/","page":"Crying Baby","title":"Crying Baby","text":"beginaligned\nO(textcry mid textfeed texthungry) = 80 \nO(textcry mid textsing texthungry) = 90 \nO(textcry mid textignore texthungry) = 80 \nO(textcry mid textfeed textsated) = 10 \nO(textcry mid textsing textsated) = 0 \nO(textcry mid textignore textsated) = 10\nendaligned","category":"page"},{"location":"crying_baby/#Reward-Function","page":"Crying Baby","title":"Reward Function","text":"","category":"section"},{"location":"crying_baby/","page":"Crying Baby","title":"Crying Baby","text":"The reward function assigns 10 reward if the baby is hungry independent of the action taken. The effort of feeding the baby adds a further 5 reward, whereas singing adds 05 reward.","category":"page"},{"location":"crying_baby/#Optimal-Infinite-Horizon-Policy","page":"Crying Baby","title":"Optimal Infinite Horizon Policy","text":"","category":"section"},{"location":"crying_baby/","page":"Crying Baby","title":"Crying Baby","text":"As baby caregivers, we seek the optimal infinite horizon policy with discount factor gamma = 09. This optimal policy is:","category":"page"},{"location":"crying_baby/","page":"Crying Baby","title":"Crying Baby","text":"(Image: Optimal Caregiver Policy)","category":"page"},{"location":"crying_baby/","page":"Crying Baby","title":"Crying Baby","text":"Note that this infinite horizon solution does not recommend singing for any belief state. However, it is optimal to sing in some finite horizon versions of this problem","category":"page"},{"location":"rock_paper_scissors/#Rock-Paper-Scissors","page":"Rock-Paper-Scissors","title":"Rock-Paper-Scissors","text":"","category":"section"},{"location":"rock_paper_scissors/#Problem-and-Action-Space","page":"Rock-Paper-Scissors","title":"Problem and Action Space","text":"","category":"section"},{"location":"rock_paper_scissors/","page":"Rock-Paper-Scissors","title":"Rock-Paper-Scissors","text":"One common game played around the world is rock-paper-scissors. There are two agents who can choose either rock, paper, or scissors. Rock beats scissors, resulting in a unit reward for the agent playing rock and a unit penalty for the agent playing scissors. Scissors beats paper, resulting in a unit reward for the agent playing scissors and a unit penalty for the agent playing paper. Finally, paper beats rock, resulting in a unit reward for the agent playing paper and a unit penalty for the agent playing rock.","category":"page"},{"location":"rock_paper_scissors/","page":"Rock-Paper-Scissors","title":"Rock-Paper-Scissors","text":"We have mathcalI = 1 2 and mathcalA = mathcalA^1 times mathcalA^2 with each mathcalA^i =  textrock textpaper textscissors.","category":"page"},{"location":"rock_paper_scissors/#Transitions-and-Rewards","page":"Rock-Paper-Scissors","title":"Transitions and Rewards","text":"","category":"section"},{"location":"rock_paper_scissors/","page":"Rock-Paper-Scissors","title":"Rock-Paper-Scissors","text":"The table below shows the rewards associated with the game, with each cell denoting R^1(a^1 a^2) R^2(a^1 a^2).","category":"page"},{"location":"rock_paper_scissors/","page":"Rock-Paper-Scissors","title":"Rock-Paper-Scissors","text":"(Image: RPS Table)","category":"page"},{"location":"rock_paper_scissors/","page":"Rock-Paper-Scissors","title":"Rock-Paper-Scissors","text":"The game can be played once or repeated any number of times. In the infinite horizon case, we use a discount factor of gamma = 09.","category":"page"},{"location":"#[DecisionMakingProblems.jl](https://github.com/algorithmsbooks/DecisionMakingProblems.jl)","page":"DecisionMakingProblems.jl","title":"DecisionMakingProblems.jl","text":"","category":"section"},{"location":"","page":"DecisionMakingProblems.jl","title":"DecisionMakingProblems.jl","text":"A Julia interface to access decision models which appeared in Algorithms for Decision Making.","category":"page"},{"location":"#Problem-Summary","page":"DecisionMakingProblems.jl","title":"Problem Summary","text":"","category":"section"},{"location":"","page":"DecisionMakingProblems.jl","title":"DecisionMakingProblems.jl","text":"The following table contains the information about each of the problems contained in this package.","category":"page"},{"location":"","page":"DecisionMakingProblems.jl","title":"DecisionMakingProblems.jl","text":"Name mathcalS mathcalA mathcalO gamma Struct Name Type\nHexworld varies 6 - 0.9 HexWorld, StraightLineHexWorld MDP\n2048 infty 4 - 1 TwentyFortyEight MDP\nCart-pole subset of mathbbR^4 2 - 1 CartPole MDP\nMountain Car subset of mathbbR^2 3 - 1 MountainCar MDP\nSimple Regulator subset of mathbbR subset of mathbbR - 1 or 0.9 LQR MDP\nAircraft collision avoidance subset of mathbbR^3 3 - 1 CollisionAvoidance MDP\nCrying baby 2 3 2 0.9 CryingBaby POMDP\nMachine Replacement 3 4 2 1 MachineReplacement POMDP\nCatch 4 10 2 0.9 Catch POMDP\nPrisoner's dilemma - 2 per agent - 1 PrisonersDilemma SimpleGame\nRock-paper-scissors - 3 per agent - 1 RockPaperScissors SimpleGame\nTraveler's Dilemma - 99 per agent - 1 Travelers SimpleGame\nPredator-prey hex world varies 6 per agent - 0.9 PredatorPreyHexWorld, CirclePredatorPreyHexWorld MG\nMultiagent Crying Baby 2 3 per agent 2 per agent 0.9 MultiCaregiverCryingBaby POMG\nCollaborative predator-prey hex world varies 6 per agent - 0.9 CollaborativePredatorPreyHexWorld, SimpleCollaborativePredatorPreyHexWorld, CircleCollaborativePredatorPreyHexWorld DecPOMDP |","category":"page"},{"location":"","page":"DecisionMakingProblems.jl","title":"DecisionMakingProblems.jl","text":"The last column has the following key:","category":"page"},{"location":"","page":"DecisionMakingProblems.jl","title":"DecisionMakingProblems.jl","text":"MDP: Markov Decision Process\nPOMDP: Partially Observable Markov Decision Process\nSimpleGame: Simple Game\nMG: Markov Game\nPOMG: Partially Observable Markov Game\nDecPOMDP: Decentralized Partially Observable Markov Decision Process","category":"page"},{"location":"#Usage","page":"DecisionMakingProblems.jl","title":"Usage","text":"","category":"section"},{"location":"","page":"DecisionMakingProblems.jl","title":"DecisionMakingProblems.jl","text":"If we look at a specific problem whose struct is named structName and whose type is type as shown in the problem summary table. Then we are able to set up an instance of the struct for that specific problem using the following:","category":"page"},{"location":"","page":"DecisionMakingProblems.jl","title":"DecisionMakingProblems.jl","text":"m = structName()\ndecprob = type(m)   # type will be the name of one of the problem types in the last column","category":"page"},{"location":"","page":"DecisionMakingProblems.jl","title":"DecisionMakingProblems.jl","text":"An example of this would be to create an instance of the Prisoner's Dilemma struct, we would use the following code:","category":"page"},{"location":"","page":"DecisionMakingProblems.jl","title":"DecisionMakingProblems.jl","text":"m = PrisonersDilemma()\ndecprob = SimpleGame(m)","category":"page"},{"location":"#MDP-Models","page":"DecisionMakingProblems.jl","title":"MDP Models","text":"","category":"section"},{"location":"","page":"DecisionMakingProblems.jl","title":"DecisionMakingProblems.jl","text":"Pages = [ \"mdp.md\", \"hexworld.md\", \"2048.md\", \"cart_pole.md\", \"mountain_car.md\", \"simple_lqr.md\", \"collision_avoidance.md\" ]","category":"page"},{"location":"#POMDP-Models","page":"DecisionMakingProblems.jl","title":"POMDP Models","text":"","category":"section"},{"location":"","page":"DecisionMakingProblems.jl","title":"DecisionMakingProblems.jl","text":"Pages = [ \"pomdp.md\", \"crying_baby.md\", \"machine_replacement.md\", \"catch.md\" ]","category":"page"},{"location":"#Simple-Games","page":"DecisionMakingProblems.jl","title":"Simple Games","text":"","category":"section"},{"location":"","page":"DecisionMakingProblems.jl","title":"DecisionMakingProblems.jl","text":"Pages = [ \"simplegame.md\", \"prisoners_dilemma.md\", \"rock_paper_scissors.md\", \"travelers.md\" ]","category":"page"},{"location":"#POMG-Models","page":"DecisionMakingProblems.jl","title":"POMG Models","text":"","category":"section"},{"location":"","page":"DecisionMakingProblems.jl","title":"DecisionMakingProblems.jl","text":"Pages = [ \"pomg.md\", \"multicaregiver.md\" ]","category":"page"},{"location":"#Markov-Games","page":"DecisionMakingProblems.jl","title":"Markov Games","text":"","category":"section"},{"location":"","page":"DecisionMakingProblems.jl","title":"DecisionMakingProblems.jl","text":"Pages = [ \"mg.md\", \"predator_prey.md\" ]","category":"page"},{"location":"#Dec-POMDP","page":"DecisionMakingProblems.jl","title":"Dec-POMDP","text":"","category":"section"},{"location":"","page":"DecisionMakingProblems.jl","title":"DecisionMakingProblems.jl","text":"Pages = [ \"decpomdp.md\", \"collab_predator_prey.md\" ]","category":"page"},{"location":"mg/#MG-Usage","page":"MG Usage","title":"MG Usage","text":"","category":"section"},{"location":"mg/#Markov-Game","page":"MG Usage","title":"Markov Game","text":"","category":"section"},{"location":"mg/","page":"MG Usage","title":"MG Usage","text":"The MG struct gives the following objects:","category":"page"},{"location":"mg/","page":"MG Usage","title":"MG Usage","text":"γ: discount factor\nℐ: agents\n𝒮: state space\n𝒜: joint action space\nT: transition function\nR: joint reward function","category":"page"},{"location":"mg/","page":"MG Usage","title":"MG Usage","text":"The agents ℐ are the players of the game. The joint action space 𝒜 is the set of all possible ordered pairs of actions amongst all of the agents. The transition function takes in a state s in 𝒮, a joint action a and a new state s' and returns the transition probability of going from s to s' by taking action a. The joint reward function R takes a state and a joint action in 𝒜 and returns a reward value.","category":"page"},{"location":"prisoners_dilemma/#Prisoner's-Dilemma","page":"Prisoner's Dilemma","title":"Prisoner's Dilemma","text":"","category":"section"},{"location":"prisoners_dilemma/#Problem-and-Action-Space","page":"Prisoner's Dilemma","title":"Problem and Action Space","text":"","category":"section"},{"location":"prisoners_dilemma/","page":"Prisoner's Dilemma","title":"Prisoner's Dilemma","text":"The prisoner’s dilemma is a classic problem in game theory involving agents with conflicting objectives. There are two prisoners that are on trial. They can choose to cooperate, remaining silent about their shared crime, or defect, blaming the other for their crime.","category":"page"},{"location":"prisoners_dilemma/","page":"Prisoner's Dilemma","title":"Prisoner's Dilemma","text":"The game has mathcalI = 1 2 and mathcalA = mathcalA^1 times mathcalA^2 with each mathcalA^i =  textcooperate textdefect.","category":"page"},{"location":"prisoners_dilemma/#Transitions-and-Rewards","page":"Prisoner's Dilemma","title":"Transitions and Rewards","text":"","category":"section"},{"location":"prisoners_dilemma/","page":"Prisoner's Dilemma","title":"Prisoner's Dilemma","text":"If they both cooperate, they both serve time for one year. If agent i cooperates and the other agent i defects, then i serves no time and i serves four years. If both defect, then they both serve three years.","category":"page"},{"location":"prisoners_dilemma/","page":"Prisoner's Dilemma","title":"Prisoner's Dilemma","text":"We use the table below to express individual rewards. Rows represent actions for agent 1. Columns represent actions for agent 2. The rewards for agent 1 and 2 are shown in each cell: R^1(a^1 a^2) R^2(a^1 a^2).","category":"page"},{"location":"prisoners_dilemma/","page":"Prisoner's Dilemma","title":"Prisoner's Dilemma","text":"(Image: Prisoners Dilemma Table)","category":"page"},{"location":"prisoners_dilemma/","page":"Prisoner's Dilemma","title":"Prisoner's Dilemma","text":"The game can be played once or repeated any number of times. In the infinite horizon case, we use a discount factor of gamma = 09.","category":"page"}]
}
