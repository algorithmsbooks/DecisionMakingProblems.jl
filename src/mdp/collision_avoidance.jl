@with_kw struct CollisionAvoidance
    ddh_max::Float64 = 1.0 # vertical acceleration limit [m/s¬≤]
    collision_threshold::Float64 = 50.0 # collision threshold [m]
    reward_collision::Float64 = -1.0 # reward obtained if collision occurs
    reward_change::Float64 = -0.01 # reward obtained if action changes
    ùíú::Vector{Float64} = [-5.0, 0.0, 5.0] # Available actions to command for vertical rate [m/s]
    # The transition distribution produces successor states
    # with dh noise with the below probabilities.
    PŒΩ::SetCategorical{Float64} = SetCategorical([2.0, 0.0, -2.0], [0.25, 0.5, 0.25])
end

struct CollisionAvoidanceState
    h::Float64 # vertical separation [m]
    dh::Float64 # rate of change of h [m/s]
    a_prev::Float64 # previous acceleration [m/s¬≤]
    œÑ::Float64 # horizontal separation time [s]
end

Base.vec(s::CollisionAvoidanceState) = [s.h, s.dh, s.a_prev, s.œÑ]

function transition(ùí´::CollisionAvoidance, s::CollisionAvoidanceState, a::Float64)
    h = s.h + s.dh
    dh = s.dh
    if a != 0.0
        if abs(a - dh) < ùí´.ddh_max
            dh += a
        else
            dh += sign(a - dh)*ùí´.ddh_max
        end
    end
    a_prev = a
    œÑ = max(s.œÑ - 1.0, -1.0)
    states = [
        CollisionAvoidanceState(h, dh + ŒΩ, a_prev, œÑ) for ŒΩ in ùí´.PŒΩ.elements
    ]
    return SetCategorical(states, ùí´.PŒΩ.distr.p)
end

is_terminal(ùí´::CollisionAvoidance, s::CollisionAvoidanceState) = s.œÑ < 0.0

function reward(ùí´::CollisionAvoidance, s::CollisionAvoidanceState, a::Float64)
    r = 0.0
    if abs(s.h) < ùí´.collision_threshold && abs(s.œÑ) < eps()
        # We collided
        r += ùí´.reward_collision
    end
    if a != s.a_prev
        # We changed our action
        r += ùí´.reward_change
    end
    return r
end

@with_kw struct CollisionAvoidanceStateDistribution
    a_prev = 0.0
    h = Uniform(-200, 200)
    dh = Uniform(-10, 10)
    tau = 40.0
end

function rand(b::CollisionAvoidanceStateDistribution)
    return CollisionAvoidanceState(Distributions.rand(b.h), Distributions.rand(b.dh), b.a_prev, b.tau)
end

@with_kw struct SimpleCollisionAvoidancePolicy
    thresh_h = 50
    thresh_œÑ = 30
    ùíú = (up = 5.0, down = -5.0, noalert = 0.0)
end

struct OptimalCollisionAvoidancePolicy
    ùíú
    grid
    Q
end

function OptimalCollisionAvoidancePolicy(mdp = CollisionAvoidance())
    ùíú = mdp.ùíú

    hs = range(-200, 200, length=21) # discretization of h in m
    dhs = range(-10, 10, length=21) # discretization of dh in m/s
    œÑs = range(0, 40, length=41) # discretization of œÑ in s

    # Discretization grid
    grid = GridInterpolations.RectangleGrid(hs, dhs, ùíú, œÑs)

    # State space
    ùíÆ = [CollisionAvoidanceState(h, dh, a_prev, œÑ) for h in hs, dh in dhs, a_prev in mdp.ùíú, œÑ in œÑs]

    # State value function
    U = zeros(length(ùíÆ))

    # State-action value function
    Q = [zeros(length(ùíÆ)) for a in mdp.ùíú]

    # Solve with backwards induction value iteration
    for (si, s) in enumerate(ùíÆ)
        for (ai, a) in enumerate(mdp.ùíú)
            Tsa = transition(mdp, s, a)
            Q[ai][si] = reward(mdp, s, a)
            Q[ai][si] += sum(is_terminal(mdp, s‚Ä≤) ? 0.0 : Tsa.distr.p[j]*GridInterpolations.interpolate(grid, U, vec(s‚Ä≤)) for (j, s‚Ä≤) in enumerate(Tsa.elements))
        end
        U[si] = maximum(q[si] for q in Q)
    end
    return OptimalCollisionAvoidancePolicy(mdp.ùíú, grid, Q)
end

function action(policy::OptimalCollisionAvoidancePolicy, s::CollisionAvoidanceState)
    vec_s = vec(s)
    a_best = first(policy.ùíú)
    q_best = -Inf
    for (a,q) in zip(policy.ùíú, policy.Q)
        q = GridInterpolations.interpolate(policy.grid, q, vec_s)
        if q > q_best
            a_best, q_best = a, q
        end
    end
    return a_best
end

function (policy::OptimalCollisionAvoidancePolicy)(s::CollisionAvoidanceState)
    return action(policy, s)
end

function action(policy::SimpleCollisionAvoidancePolicy, s::CollisionAvoidanceState)
    if abs(s.h) < policy.thresh_h && s.œÑ < policy.thresh_œÑ
        return (s.h > 0.0) ? policy.ùíú.up : policy.ùíú.down
    end
    return policy.ùíú.noalert
end

function (policy::SimpleCollisionAvoidancePolicy)(s::CollisionAvoidanceState)
    return action(policy, s)
end


struct CollisionAvoidanceValueFunction
    grid
    U
end

function CollisionAvoidanceValueFunction(ùí´::CollisionAvoidance, policy)
    ùíú = ùí´.ùíú

    hs = range(-200, 200, length=21) # discretization of h in m
    dhs = range(-10, 10, length=21) # discretization of dh in m/s
    œÑs = range(0, 40, length=41) # discretization of œÑ in s

    # Discretization grid
    grid = GridInterpolations.RectangleGrid(hs, dhs, ùíú, œÑs)

    # State space
    ùíÆ = [CollisionAvoidanceState(h, dh, a_prev, œÑ) for h in hs, dh in dhs, a_prev in ùí´.ùíú, œÑ in œÑs]

    # State value function
    U = zeros(length(ùíÆ))

    # Solve with backwards induction value iteration
    for (si, s) in enumerate(ùíÆ)
        a = action(policy, s)
        Tsa = transition(ùí´, s, a)
        q = reward(ùí´, s, a)
        q += sum(is_terminal(ùí´, s‚Ä≤) ? 0.0 : Tsa.distr.p[j]*GridInterpolations.interpolate(grid, U, vec(s‚Ä≤)) for (j, s‚Ä≤) in enumerate(Tsa.elements))
        U[si] = q
    end
    return CollisionAvoidanceValueFunction(grid, U)
end

function (U::CollisionAvoidanceValueFunction)(s)
    return GridInterpolations.interpolate(U.grid, U.U, vec(s))
end

function MDP(mdp::CollisionAvoidance; Œ≥::Float64=1.0)
    return MDP(
            Œ≥,
            nothing, # no ordered states
            mdp.ùíú,
            (s,a) -> transition(mdp,s,a), # no probabilistic transition function
            (s,a) -> reward(mdp, s, a),
            (s,a)->begin
                s‚Ä≤ = rand(transition(mdp,s,a))
                r = reward(mdp, s, a)
                return (s‚Ä≤, r)
            end
        )
end
