@with_kw struct CollisionAvoidanceMDP
    ddh_max::Float64 = 1.0 # vertical acceleration limit [m/sÂ²]
    collision_threshold::Float64 = 50.0 # collision threshold [m]
    reward_collision::Float64 = -1.0 # reward obtained if collision occurs
    reward_change::Float64 = -0.01 # reward obtained if action changes
    ğ’œ::Vector{Float64} = [-5.0, 0.0, 5.0] # Available actions to command for vertical rate [m/s]
    # The transition distribution produces successor states
    # with dh noise with the below probabilities.
    PÎ½::SetCategorical{Float64} = SetCategorical([2.0, 0.0, -2.0], [0.25, 0.5, 0.25])
end

struct CollisionAvoidanceMDPState
    h::Float64 # vertical separation [m]
    dh::Float64 # rate of change of h [m/s]
    a_prev::Float64 # previous acceleration [m/sÂ²]
    Ï„::Float64 # horizontal separation time [s]
end

Base.vec(s::CollisionAvoidanceMDPState) = [s.h, s.dh, s.a_prev, s.Ï„]

function transition(ğ’«::CollisionAvoidanceMDP, s::CollisionAvoidanceMDPState, a::Float64)
    h = s.h + s.dh
    dh = s.dh
    if a != 0.0
        if abs(a - dh) < ğ’«.ddh_max
            dh += a
        else
            dh += sign(a - dh)*ğ’«.ddh_max
        end
    end
    a_prev = a
    Ï„ = max(s.Ï„ - 1.0, -1.0)
    states = [
        CollisionAvoidanceMDPState(h, dh + Î½, a_prev, Ï„) for Î½ in ğ’«.PÎ½.elements
    ]
    return SetCategorical(states, ğ’«.PÎ½.distr.p)
end

is_terminal(ğ’«::CollisionAvoidanceMDP, s::CollisionAvoidanceMDPState) = s.Ï„ < 0.0

function reward(ğ’«::CollisionAvoidanceMDP, s::CollisionAvoidanceMDPState, a::Float64)
    r = 0.0
    if abs(s.h) < ğ’«.collision_threshold && abs(s.Ï„) < eps()
        # We collided
        r += ğ’«.reward_collision
    end
    if a != s.a_prev
        # We changed our action
        r += ğ’«.reward_change
    end
    return r
end

@with_kw struct CollisionAvoidanceStateDistribution
    a_prev = 0.0
    h = Uniform(-200, 200)
    dh = Uniform(-10, 10)
    tau = 40.0
end

function rand(b::CollisionAvoidanceStateDistribution)
    return CollisionAvoidanceMDPState(Distributions.rand(b.h), Distributions.rand(b.dh), b.a_prev, b.tau)
end

@with_kw struct SimpleCollisionAvoidancePolicy
    thresh_h = 50
    thresh_Ï„ = 30
    ğ’œ = (up = 5.0, down = -5.0, noalert = 0.0)
end

struct OptimalCollisionAvoidancePolicy
    ğ’œ
    grid
    Q
end

function OptimalCollisionAvoidancePolicy(mdp = CollisionAvoidanceMDP())
    ğ’œ = mdp.ğ’œ

    hs = range(-200, 200, length=21) # discretization of h in m
    dhs = range(-10, 10, length=21) # discretization of dh in m/s
    Ï„s = range(0, 40, length=41) # discretization of Ï„ in s

    # Discretization grid
    grid = GridInterpolations.RectangleGrid(hs, dhs, ğ’œ, Ï„s)

    # State space
    ğ’® = [CollisionAvoidanceMDPState(h, dh, a_prev, Ï„) for h in hs, dh in dhs, a_prev in mdp.ğ’œ, Ï„ in Ï„s]

    # State value function
    U = zeros(length(ğ’®))

    # State-action value function
    Q = [zeros(length(ğ’®)) for a in mdp.ğ’œ]

    # Solve with backwards induction value iteration
    for (si, s) in enumerate(ğ’®)
        for (ai, a) in enumerate(mdp.ğ’œ)
            Tsa = transition(mdp, s, a)
            Q[ai][si] = reward(mdp, s, a)
            Q[ai][si] += sum(is_terminal(mdp, sâ€²) ? 0.0 : Tsa.distr.p[j]*GridInterpolations.interpolate(grid, U, vec(sâ€²)) for (j, sâ€²) in enumerate(Tsa.elements))
        end
        U[si] = maximum(q[si] for q in Q)
    end
    return OptimalCollisionAvoidancePolicy(mdp.ğ’œ, grid, Q)
end

function action(policy::OptimalCollisionAvoidancePolicy, s::CollisionAvoidanceMDPState)
    vec_s = vec(s)
    a_best = first(policy.ğ’œ)
    q_best = -Inf
    for (a,q) in zip(policy.ğ’œ, policy.Q)
        q = GridInterpolations.interpolate(policy.grid, q, vec_s)
        if q > q_best
            a_best, q_best = a, q
        end
    end
    return a_best
end

function (policy::OptimalCollisionAvoidancePolicy)(s::CollisionAvoidanceMDPState)
    return action(policy, s)
end

function action(policy::SimpleCollisionAvoidancePolicy, s::CollisionAvoidanceMDPState)
    if abs(s.h) < policy.thresh_h && s.Ï„ < policy.thresh_Ï„
        return (s.h > 0.0) ? policy.ğ’œ.up : policy.ğ’œ.down
    end
    return policy.ğ’œ.noalert
end

function (policy::SimpleCollisionAvoidancePolicy)(s::CollisionAvoidanceMDPState)
    return action(policy, s)
end


struct CollisionAvoidanceValueFunction
    grid
    U
end

function CollisionAvoidanceValueFunction(ğ’«::CollisionAvoidanceMDP, policy)
    ğ’œ = ğ’«.ğ’œ

    hs = range(-200, 200, length=21) # discretization of h in m
    dhs = range(-10, 10, length=21) # discretization of dh in m/s
    Ï„s = range(0, 40, length=41) # discretization of Ï„ in s

    # Discretization grid
    grid = GridInterpolations.RectangleGrid(hs, dhs, ğ’œ, Ï„s)

    # State space
    ğ’® = [CollisionAvoidanceMDPState(h, dh, a_prev, Ï„) for h in hs, dh in dhs, a_prev in ğ’«.ğ’œ, Ï„ in Ï„s]

    # State value function
    U = zeros(length(ğ’®))

    # Solve with backwards induction value iteration
    for (si, s) in enumerate(ğ’®)
        a = action(policy, s)
        Tsa = transition(ğ’«, s, a)
        q = reward(ğ’«, s, a)
        q += sum(is_terminal(ğ’«, sâ€²) ? 0.0 : Tsa.distr.p[j]*GridInterpolations.interpolate(grid, U, vec(sâ€²)) for (j, sâ€²) in enumerate(Tsa.elements))
        U[si] = q
    end
    return CollisionAvoidanceValueFunction(grid, U)
end

function (U::CollisionAvoidanceValueFunction)(s)
    return GridInterpolations.interpolate(U.grid, U.U, vec(s))
end
